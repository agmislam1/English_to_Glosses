{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Package installation and import\n"
      ],
      "metadata": {
        "id": "TrgOaWFiPfuB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tnxXKDjq3jEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1dc40d-a465-4100-f3cf-973bcee1bc5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install \"tensorflow-text==2.8.*\"\n",
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "### Data loading\n",
        "\n",
        "I have uploaded the data in the my colab and read the data from there.\n",
        "\n",
        "I read the data from source and the target file line by line. newline is the separator for the line. Then seggreagate the words by spearating using the separator whitespace and comma for source and target files respectively.\n",
        "\n",
        "For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
        "\n",
        "1. Add a *start* and *end* token to each sentence.\n",
        "2. Clean the sentences by removing special characters.\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_file():\n",
        "  source_file = \"sentencesTrain.txt\"\n",
        "  target_file = \"tokensTrain.txt\"\n",
        "\n",
        "  text_pairs_source = []\n",
        "  text_pairs_target = []\n",
        "\n",
        "  source_vocab = []\n",
        "  target_vocab = []\n",
        "\n",
        "  max_seq_inp = 0\n",
        "  max_seq_tar = 0\n",
        "\n",
        "  with open(source_file,encoding=\"unicode_escape\") as file1, open(target_file,encoding=\"unicode_escape\") as file2:\n",
        "      for line1, line2 in zip(file1, file2):\n",
        "          line1 = line1.replace(\"\\n\", \" \")\n",
        "          line1 = line1.strip()\n",
        "\n",
        "          \n",
        "          \n",
        "          line2 = line2.replace(\",\", \" \")\n",
        "          line2 = line2.replace(\"\\n\", \" \")\n",
        "          line2 = line2.strip()\n",
        "          text_pairs_source.append(line1)\n",
        "          text_pairs_target.append(line2)\n",
        "          \n",
        "          # Finding the max sequence\n",
        "          seq_i = line1.split(\" \")\n",
        "          seq_t = line2.split(\" \")\n",
        "\n",
        "          for word in seq_i:\n",
        "              source_vocab.append(word)\n",
        "          for word in seq_t:\n",
        "              target_vocab.append(word)\n",
        "\n",
        "          \n",
        "\n",
        "          if len(seq_i)>max_seq_inp:\n",
        "              max_seq_inp = len(seq_i)         \n",
        "            \n",
        "            \n",
        "          if len(seq_t)>max_seq_tar:\n",
        "              max_seq_tar = len(seq_t)\n",
        "\n",
        "\n",
        "  return text_pairs_target,text_pairs_source,max_seq_inp,max_seq_tar,source_vocab,target_vocab"
      ],
      "metadata": {
        "id": "Ye1urmMzAxNt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for counting vocabulary size"
      ],
      "metadata": {
        "id": "Pgb6X9eu66wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unique(list1):\n",
        "     \n",
        "    # insert the list to the set\n",
        "    list_set = set(list1)\n",
        "    # convert the set to the list\n",
        "    unique_list = (list(list_set))\n",
        "    counter = 0\n",
        "    for x in unique_list:\n",
        "        counter = counter +1\n",
        "    return counter\n",
        "      "
      ],
      "metadata": {
        "id": "fIGJXyQ3TqRC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis\n",
        "\n",
        "\n",
        "\n",
        "*   Vocabulary Size\n",
        "*   Maximum Sequence\n",
        "*   Sample data before tokenization\n",
        "\n"
      ],
      "metadata": {
        "id": "NVt9cHiVQOHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targ, inp, s_len, t_len, source_vocab,target_vocab = load_data_file()\n",
        "\n",
        " \n",
        "\n",
        "max_vocab_size_source = unique(source_vocab)\n",
        "max_vocab_size_target = unique(target_vocab)\n",
        "print(\"Vocabulary Size for Source: \" + str(max_vocab_size_source))\n",
        "print(\"Vocabulary Size for Target: \" + str(max_vocab_size_target))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_xh8QUodBi6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c824f6-08cf-4999-cb8a-5909b46fa93b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size for Source: 1552\n",
            "Vocabulary Size for Target: 1291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Maximum Sequence in Source: \"+ str(s_len))\n",
        "print(\"Maximum Sequence in Target: \"+ str(t_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPX_orLR8VKS",
        "outputId": "9f268b25-3b15-4da0-e799-15bae23d6428"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Sequence in Source: 29\n",
            "Maximum Sequence in Target: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "for i in range(5):\n",
        "    print(\"Source \" + str(i) + \":\" + random.choice(inp))\n",
        "    print(\"Target \" + str(i) + \":\" + random.choice(targ))\n",
        "    print(\"----------------------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cUws-qEO_4D_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883eeb67-d6a2-4b68-b9f4-c70f901f178f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source 0:What is my aunt an expert at making?\n",
            "Target 0:(25)WHY FATHER (P)UPSET (1h)part:indef KNOW+(1h)NEG\n",
            "----------------------------------------------\n",
            "Source 1:When I meet new people Im lousy at memorizing their names.\n",
            "Target 1:fs-JOHN IX-3p:i NOT BUY TURKEY (1h)part:indef BUY\n",
            "----------------------------------------------\n",
            "Source 2:Why dont you like that video game?\n",
            "Target 2:IX-2p ARTICLE IX-2p WRITE FOR ns-BOSTON fs-GLOBE IX-1p READ WOW (1h)GOOD/THANK-YOU IX-3p:i ARTICLE WRITE READ\n",
            "----------------------------------------------\n",
            "Source 3:I just drank it and it tastes fine.\n",
            "Target 3:HEAR/LISTEN TEACH+AGENT WHO TEACH AGENT\n",
            "----------------------------------------------\n",
            "Source 4:Are Jen and Joe married?\n",
            "Target 4:IX-2p GO WHERE (1h)part:indef GO\n",
            "----------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tensorflow dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data after converting to tensorflow"
      ],
      "metadata": {
        "id": "Bn7QMLsW8tJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qc6-NK1GtWQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa9fcbf-4045-485e-95b4-a8ef0d5efeb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Once my friends start dying their hair, many more people will follow.'\n",
            " b'Who bought the car?'\n",
            " b\"No, it won't rain, but if it does the game will be cancelled for sure.\"\n",
            " b'My pay increased a little bit.'\n",
            " b'Each student that received an award is here watching.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'#WHEN POSS-1p FRIEND START HAIR_2+DYE etcetera GUARANTEE MANY PEOPLE FUTURE FOLLOW FRIEND START HAIR_2 DYE etcetera GUARANTEE MANY PEOPLE FOLLOW'\n",
            " b'WHO BUY CAR part:indef BUY CAR part:indef'\n",
            " b'wave no REFUSE RAIN you see #IF+ RAIN you know GAME+ FOR REALLY CANCEL/CRITICIZE RAIN you see RAIN you know GAME+ CANCEL/CRITICIZE'\n",
            " b'POSS-1p PAY IX-1p INCREASE LITTLE-BIT SMALL-AMOUNT INCREASE part:indef PAY INCREASE part:indef'\n",
            " b'EACH STUDENT WHO GET TROPHY IX-3p:i HERE (2h)LOOK-UP EACH STUDENT GET TROPHY HERE (2h)LOOK-UP'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOQ5n55X4uDB"
      },
      "source": [
        "#### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD0e-DWGQ2Vo"
      },
      "outputs": [],
      "source": [
        "example_text = tf.constant('When does the party start, at 8 or 9?')\n",
        "\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hTllEjK6RSo"
      },
      "source": [
        "**Source data preprocessing**\n",
        "\n",
        "Built in library is used\n",
        "\n",
        "1.   Unicode normalization\n",
        "2.   Replcae unwanted characters\n",
        "3.   Add start and end token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "chTF5N885F0P"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ 0-9 a-z.?!,¿]', '')\n",
        "  \n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target data preprocessing**\n",
        "\n",
        "Built in library is used\n",
        "1.   Add start and end token\n",
        "\n",
        "Since I am not aware of the sign language, I did not make any modification to the target data."
      ],
      "metadata": {
        "id": "SS3DKfr_9h1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_normalize_tex(text):\n",
        "\n",
        "  \n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "tq08zXUIJteJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-sKsSI7xRZ"
      },
      "source": [
        "#### Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aKn8qd37abi"
      },
      "source": [
        "I used built in function to vectorize the data for both source and the target."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 500"
      ],
      "metadata": {
        "id": "d46rbjM_98qU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size_source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kbC6ODP8IK_"
      },
      "source": [
        "Sample source data after vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bmsI1Yql8FYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebad59f-aee2-4926-a173-cd32db78e675"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'the', ',', 'i', 'to', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGjIFjX8_Wp"
      },
      "source": [
        "Sample target data after vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "jlC4xuZnKLBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd1e98b-8230-4d52-ab33-cabecae7c9f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " '[START]',\n",
              " '[END]',\n",
              " 'IX-1p',\n",
              " 'FRIEND',\n",
              " 'IX-3p:i',\n",
              " 'part:indef',\n",
              " 'IX-loc:i',\n",
              " '(1h)part:indef']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_normalize_tex,\n",
        "    #standardize = tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size_target)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary(targ)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQqlP_s9eIv"
      },
      "source": [
        "Convert the strings into token ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9KZxj8IrNZ9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00faf89a-0fe8-413e-8dae-5cb3a02943c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[  2, 373,  10,  30,  71, 980,  74, 397,   6, 160],\n",
              "       [  2,  18, 138,   5,  60,   9,   3,   0,   0,   0],\n",
              "       [  2,  21,   6,  16, 128, 206,   6,  94,  11,  16]])>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert token ID to original word"
      ],
      "metadata": {
        "id": "pQvmc1YS-zBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "98g9rcxGQY0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ded2447a-16be-4b6c-e57b-8bb87b415744"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] once my friends start dying their hair , many more people will follow . [END]          '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot0aCL9t-Ghi"
      },
      "source": [
        "The returned token IDs are zero-padded. This can easily be turned into a mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "_jx4Or_eFRSz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "eda8c644-da15-4217-9cf4-2fb6a885dfbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzQWx2saImMV"
      },
      "source": [
        "Define the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "_a9uNz3-IrF-"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "units = 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_builtins = True\n",
        "\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "metadata": {
        "id": "gvqzgGCdAINy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNgVbLSzpsr"
      },
      "source": [
        "### The encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "60gSVh05Jl6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72d5118-bbc6-478f-a418-6ef9e71bb5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch, shape (batch): (64,)\n",
            "Input batch tokens, shape (batch, s): (64, 26)\n",
            "Encoder output, shape (batch, s, units): (64, 26, 1024)\n",
            "Encoder state, shape (batch, units): (64, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The attention head"
      ],
      "metadata": {
        "id": "DqcNX6bkASRk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "momiE59lXo6U"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf13LubPGjDO"
      },
      "source": [
        "### Test the Attention layer\n",
        "\n",
        "Create a `BahdanauAttention` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "t4QMlOp8Gidh"
      },
      "outputs": [],
      "source": [
        "attention_layer = BahdanauAttention(units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snA1uL9AI-JE"
      },
      "source": [
        "This layer takes 3 inputs:\n",
        "\n",
        "* The query This will be generated by the decoder, later.\n",
        "* The value: This Will be the output of the encoder.\n",
        "* The mask: To exclude the padding, example_tokens != 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "DYSHqmORgVFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9461cd4a-0d2f-42b8-c348-6a524703af23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "(example_tokens != 0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "7y7hjPkNMmHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ee1beb-6dae-4578-adeb-1d4c5b28717c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (64, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 26)\n"
          ]
        }
      ],
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AagyXMH-Jhqt"
      },
      "source": [
        "The attention weights must sum to `1.0` for each sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ZuzrCdmYlTcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c16b44b-208e-4517-c481-11d01ee70dcf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 2, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "attention_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qIMwC-f-ZC8N"
      },
      "outputs": [],
      "source": [
        "attention_slice = attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysWDPO6hOS8X"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ638eHN4iCK"
      },
      "source": [
        "### The decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "erYvHIgAl8kh"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "7WfSIb2sArRT"
      },
      "outputs": [],
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NChkl2KrnV2y"
      },
      "source": [
        "Here is the implementation of the call method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "PJOi5btHAPNK"
      },
      "outputs": [],
      "source": [
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "  shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "  shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "  if state is not None:\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 1. Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "  shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "  # Step 2. Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 3. Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "  shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "  shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "  # Step 5. Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "  shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Ay_mTMPfnb2a"
      },
      "outputs": [],
      "source": [
        "Decoder.call = call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "4ZUMbYXIEVeA"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnaw583CpnY"
      },
      "source": [
        "The decoder takes 4 inputs (new_tokens, enc_output, mask, state).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "4u6eJBU4GL40"
      },
      "outputs": [],
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "example_output_tokens = output_text_processor(example_target_batch)\n",
        "\n",
        "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "E5hqvbR5FUCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14468a88-77c8-4e38-9cdf-1ff4ee13f89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape: (batch_size, t, output_vocab_size) (64, 1, 1291)\n",
            "state shape: (batch_size, dec_units) (64, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Run the decoder\n",
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(new_tokens=first_token,\n",
        "                          enc_output=example_enc_output,\n",
        "                          mask=(example_tokens != 0)),\n",
        "    state = example_enc_state\n",
        ")\n",
        "\n",
        "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "outputs": [],
      "source": [
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKXTLYu4IV7I"
      },
      "outputs": [],
      "source": [
        "vocab = np.array(output_text_processor.get_vocabulary())\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX1VF9XDJTOM"
      },
      "outputs": [],
      "source": [
        "dec_result, dec_state = decoder(\n",
        "    DecoderInput(sampled_token,\n",
        "                 example_enc_output,\n",
        "                 mask=(example_tokens != 0)),\n",
        "    state=dec_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1rs0XL7Y2aS"
      },
      "outputs": [],
      "source": [
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6xyru86m914"
      },
      "source": [
        "## Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "WWIyuy71TkJT"
      },
      "outputs": [],
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ZlYE68wzXoA8"
      },
      "outputs": [],
      "source": [
        "def _preprocess(self, input_text, target_text):\n",
        "  self.shape_checker(input_text, ('batch',))\n",
        "  self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "  self.shape_checker(input_tokens, ('batch', 's'))\n",
        "  self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "  self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "  self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "lHy6hzStrgjQ"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._preprocess = _preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Qs_gsISsYPpY"
      },
      "outputs": [],
      "source": [
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs  \n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "KGwWHIxLrjGR"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._train_step = _train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9VrzgwztXzYJ"
      },
      "outputs": [],
      "source": [
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "  self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "  self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "  self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "xj3I7VULrk1R"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._loop_step = _loop_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WACCHvKWBQ9C"
      },
      "source": [
        "### Test the training step\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "OA6bCske8TXm"
      },
      "outputs": [],
      "source": [
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "zHe-OudqCFGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e41b907-15d8-4b35-93bf-30fde9716351"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.1631723908466425"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "np.log(output_text_processor.vocabulary_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "UFUsTKQx0jaH"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "2-bgU59jrztQ"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "KC8bRv_Gr3H9"
      },
      "outputs": [],
      "source": [
        "translator.use_tf_function = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLQZsX2dp1QK"
      },
      "outputs": [],
      "source": [
        "translator.train_step([example_input_batch, example_target_batch])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI02XFjoEt1k"
      },
      "source": [
        "### Create the model using encoder, attention, and decoder\n",
        "I use \"Adam\" as optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Emgfgh4tAmJt"
      },
      "outputs": [],
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "I use a callbacks.Callback to collect the history of batch losses, for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "J7m4mtnj80sq"
      },
      "outputs": [],
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the number of epochs"
      ],
      "metadata": {
        "id": "6rZor9HSDJIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 40"
      ],
      "metadata": {
        "id": "2f077Ut_DIZn"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "BQd_esVVoSf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dacd106-8264-4e9e-c79e-c0e24798657e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "11/11 [==============================] - 13s 1s/step - batch_loss: 3.6649\n",
            "Epoch 2/40\n",
            "11/11 [==============================] - 10s 916ms/step - batch_loss: 3.3462\n",
            "Epoch 3/40\n",
            "11/11 [==============================] - 10s 903ms/step - batch_loss: 3.0367\n",
            "Epoch 4/40\n",
            "11/11 [==============================] - 10s 931ms/step - batch_loss: 2.7174\n",
            "Epoch 5/40\n",
            "11/11 [==============================] - 10s 914ms/step - batch_loss: 2.4467\n",
            "Epoch 6/40\n",
            "11/11 [==============================] - 10s 924ms/step - batch_loss: 2.1591\n",
            "Epoch 7/40\n",
            "11/11 [==============================] - 10s 921ms/step - batch_loss: 1.9167\n",
            "Epoch 8/40\n",
            "11/11 [==============================] - 10s 906ms/step - batch_loss: 1.7022\n",
            "Epoch 9/40\n",
            "11/11 [==============================] - 10s 941ms/step - batch_loss: 1.4866\n",
            "Epoch 10/40\n",
            "11/11 [==============================] - 10s 922ms/step - batch_loss: 1.2899\n",
            "Epoch 11/40\n",
            "11/11 [==============================] - 10s 914ms/step - batch_loss: 1.0912\n",
            "Epoch 12/40\n",
            "11/11 [==============================] - 10s 911ms/step - batch_loss: 0.9143\n",
            "Epoch 13/40\n",
            "11/11 [==============================] - 10s 911ms/step - batch_loss: 0.8168\n",
            "Epoch 14/40\n",
            "11/11 [==============================] - 10s 897ms/step - batch_loss: 0.7115\n",
            "Epoch 15/40\n",
            "11/11 [==============================] - 10s 913ms/step - batch_loss: 0.5851\n",
            "Epoch 16/40\n",
            "11/11 [==============================] - 10s 925ms/step - batch_loss: 0.5004\n",
            "Epoch 17/40\n",
            "11/11 [==============================] - 10s 910ms/step - batch_loss: 0.4124\n",
            "Epoch 18/40\n",
            "11/11 [==============================] - 10s 889ms/step - batch_loss: 0.3402\n",
            "Epoch 19/40\n",
            "11/11 [==============================] - 10s 929ms/step - batch_loss: 0.2902\n",
            "Epoch 20/40\n",
            "11/11 [==============================] - 12s 1s/step - batch_loss: 0.2347\n",
            "Epoch 21/40\n",
            "11/11 [==============================] - 13s 1s/step - batch_loss: 0.1955\n",
            "Epoch 22/40\n",
            "11/11 [==============================] - 10s 917ms/step - batch_loss: 0.1667\n",
            "Epoch 23/40\n",
            "11/11 [==============================] - 11s 966ms/step - batch_loss: 0.1419\n",
            "Epoch 24/40\n",
            "11/11 [==============================] - 10s 884ms/step - batch_loss: 0.1219\n",
            "Epoch 25/40\n",
            "11/11 [==============================] - 10s 907ms/step - batch_loss: 0.1056\n",
            "Epoch 26/40\n",
            "11/11 [==============================] - 10s 922ms/step - batch_loss: 0.0933\n",
            "Epoch 27/40\n",
            "11/11 [==============================] - 13s 1s/step - batch_loss: 0.0827\n",
            "Epoch 28/40\n",
            "11/11 [==============================] - 10s 942ms/step - batch_loss: 0.0783\n",
            "Epoch 29/40\n",
            "11/11 [==============================] - 16s 1s/step - batch_loss: 0.0693\n",
            "Epoch 30/40\n",
            "11/11 [==============================] - 12s 1s/step - batch_loss: 0.0617\n",
            "Epoch 31/40\n",
            "11/11 [==============================] - 10s 875ms/step - batch_loss: 0.0565\n",
            "Epoch 32/40\n",
            "11/11 [==============================] - 10s 895ms/step - batch_loss: 0.0502\n",
            "Epoch 33/40\n",
            "11/11 [==============================] - 12s 1s/step - batch_loss: 0.0478\n",
            "Epoch 34/40\n",
            "11/11 [==============================] - 12s 1s/step - batch_loss: 0.0428\n",
            "Epoch 35/40\n",
            "11/11 [==============================] - 10s 924ms/step - batch_loss: 0.0399\n",
            "Epoch 36/40\n",
            "11/11 [==============================] - 10s 932ms/step - batch_loss: 0.0396\n",
            "Epoch 37/40\n",
            "11/11 [==============================] - 10s 915ms/step - batch_loss: 0.0347\n",
            "Epoch 38/40\n",
            "11/11 [==============================] - 10s 908ms/step - batch_loss: 0.0354\n",
            "Epoch 39/40\n",
            "11/11 [==============================] - 10s 925ms/step - batch_loss: 0.0316\n",
            "Epoch 40/40\n",
            "11/11 [==============================] - 10s 903ms/step - batch_loss: 0.0300\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc734e5c510>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "train_translator.fit(dataset, epochs=num_epoch,\n",
        "                     callbacks=[batch_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "38rLdlmtQHCm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "b39934e3-1f96-4591-efac-48e2346cfb7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'CE/token')"
            ]
          },
          "metadata": {},
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0S_O_RzHmfe"
      },
      "source": [
        "There is a fluctuation in the between the batches of the epochs initially and then it smoothly converges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "PO-CLL1LVBbM"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "iBQzFZ9uWU79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195e502d-0f28-4878-de7c-17572454ceae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ],
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59PN-UxqYrU"
      },
      "source": [
        "### Convert token IDs to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "8IjwKTwtmdFf"
      },
      "outputs": [],
      "source": [
        "def tokens_to_text(self, result_tokens):\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(result_tokens, ('batch', 't'))\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "  shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "  shape_checker(result_text, ('batch'))\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  shape_checker(result_text, ('batch',))\n",
        "  return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "912aV0K7r90w"
      },
      "outputs": [],
      "source": [
        "Translator.tokens_to_text = tokens_to_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krBuAapkqNs9"
      },
      "source": [
        "Input some random token IDs and see what it generates:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC9De_kAqtaE"
      },
      "source": [
        "### Sample from the decoder's predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "8lfuj3GcdD6e"
      },
      "outputs": [],
      "source": [
        "def sample(self, logits, temperature):\n",
        "  shape_checker = ShapeChecker()\n",
        "  # 't' is usually 1 here.\n",
        "  shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "  shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "  shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "  \n",
        "  shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "  return new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "4DpDnBdBdL9_"
      },
      "outputs": [],
      "source": [
        "Translator.sample = sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwdHfGEfsmy5"
      },
      "source": [
        "Test run this function on some random inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "rwLT0nxXym80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dba6be5-c845-469d-8f9e-8f5300a0f0cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
              "array([[ 421],\n",
              "       [1201],\n",
              "       [ 930],\n",
              "       [ 104],\n",
              "       [ 712]])>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
        "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
        "example_output_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation\n",
        "\n",
        "*   Prediction is stored in list\n",
        "*   Then it is converted to tensors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NosY-YJKEiem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "ZmOvVrZmwAxg"
      },
      "outputs": [],
      "source": [
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "    \n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention.append(dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "JOmd8Y269MG3"
      },
      "outputs": [],
      "source": [
        "Translator.translate = translate_unrolled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxYXf3GNKKLS"
      },
      "source": [
        "### Check the translation from the training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "input_text = tf.constant([\n",
        "    'If mom has time she will bring cookies.', # \"MOTHER,IX-loc:i,#IF+,(1h)HAVE,TIME+,IX-3p:i,FUTURE,BRING,COOKIE,TIME+,BRING,COOKIE\"\n",
        "    \"I just drank it and it tastes fine.\", # \"IX-1p,RECENT-PAST,DRINK,TASTE,FINE++,(1h)part:indef\"\n",
        "    \"My friend goes out to a party.\", # \"FRIEND,GO-OUT,(P)PARTY,FRIEND,(P)PARTY\"\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print(result['text'][2].numpy().decode())\n"
      ],
      "metadata": {
        "id": "HnP8yc69AqCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c9af64d-79fd-41c1-edf7-12ede9b46275"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#IF POSS-1p FRIEND IX-3p:i FINISH BRING COOKIE GUARANTEE BRING COOKIE\n",
            "IX-1p RECENT-PAST DRINK TASTE FINE++ (1h)part:indef\n",
            "FRIEND IX-3p:i GO-OUT (P)PARTY+ FRIEND DRIVE\n",
            "CPU times: user 517 ms, sys: 13.3 ms, total: 530 ms\n",
            "Wall time: 528 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-6cFyqeUPQm"
      },
      "source": [
        "### Output the predicted tokens from the test data in a comma sepearted txt files.\n",
        "\n",
        "*   Collected the predicted result for each line\n",
        "*   Replace the whitespace with comma\n",
        "*   Finally write the text to file by line \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "version = \"v4\"\n",
        "test_file = \"sentencesTest.txt\"\n",
        "\n",
        "outputfile = \"submission_islam_\"+version+\".txt\"\n",
        "out = open(outputfile, \"w\")\n",
        "\n",
        "with open(test_file,encoding=\"unicode_escape\" ) as f:\n",
        "    lines_test = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "for input_sentence in lines_test:\n",
        "    \n",
        "    list_data = []\n",
        "    list_data.append(input_sentence)\n",
        "    input_text = tf.constant(list_data)\n",
        "    result = translator.translate(input_text = input_text)\n",
        "\n",
        "    \n",
        "    translated = result['text'][0].numpy().decode()\n",
        "    translated = translated.replace(\" \", \",\")\n",
        "    \n",
        "    \n",
        "    out.write(translated+'\\n')\n",
        "\n",
        "out.close()"
      ],
      "metadata": {
        "id": "D1xCCsZ_PRWP"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference:**\n",
        "\n",
        "* https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "\n",
        "I have used this model which was used to create translation from English to Spanish.\n"
      ],
      "metadata": {
        "id": "_y52x-Z7KN3x"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "sign_nmt_with_attention_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}